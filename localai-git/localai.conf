# Configuration file for the application

TMPDIR=/STORAGE/local-ai
LOCALAI_BACKEND_ASSETS=/STORAGE/local-ai/backend_data
LOCALAI_TMP_DIR=/STORAGE/local-ai/tmp
LOCALAI_WORKER_DIR=/STORAGE/local-ai/workers

## Specify a different bind address (defaults to ":8080")
LOCALAI_ADDRESS=10.0.0.212:9461

## Default models context size
LOCALAI_CONTEXT_SIZE=10240

## Define galleries.
## models will to install will be visible in `/models/available`
LOCALAI_GALLERIES=[{"name":"localai", "url":"github:mudler/LocalAI/gallery/index.yaml@master"}, {"name":"skynet", "url":"github:go-skynet/model-gallery/index.yaml"}, {"name":"Huggingface", "url": "github:go-skynet/model-gallery/huggingface.yaml"}]

# Enable GPU acceleration
F16=true

## Set number of threads. defaults to physical cores (since bdd6769)
## Number of threads used for parallel computation. Usage of the number of physical cores in the system is suggested.
LOCALAI_THREADS=12

## CORS settings
# LOCALAI_CORS=true
# LOCALAI_CORS_ALLOW_ORIGINS=*

## Enable debug mode
# LOCALAI_LOG_LEVEL=debug

## Disables COMPEL (Diffusers)
# COMPEL=0

## Enable/Disable single backend (useful if only one GPU is available)
LOCALAI_SINGLE_ACTIVE_BACKEND=true

## Specify a default upload limit in MB (whisper)
LOCALAI_UPLOAD_LIMIT=100

## List of external GRPC backends (note on the container image this variable is already set to use extra backends available in extra/)
LOCALAI_EXTERNAL_GRPC_BACKENDS=my-backend:127.0.0.1:9000,my-backend2:/usr/bin/backend.py

### Advanced settings ###
### Those are not really used by LocalAI, but from components in the stack ###
##
### Preload libraries
# LD_PRELOAD=
LD_LIBRARY_PATH=/usr/lib

### Huggingface cache for models
HUGGINGFACE_HUB_CACHE=/STORAGE/local-ai/huggingface

### Python backends GRPC max workers
### Default number of workers for GRPC Python backends.
### This actually controls wether a backend can process multiple requests or not.
PYTHON_GRPC_MAX_WORKERS=12

### Define the number of parallel LLAMA.cpp workers (Defaults to 1)
LLAMACPP_PARALLEL=12

### Define a list of GRPC Servers for llama-cpp workers to distribute the load
# https://github.com/ggerganov/llama.cpp/pull/6829
# https://github.com/ggerganov/llama.cpp/blob/master/examples/rpc/README.md
# LLAMACPP_GRPC_SERVERS=""

### Enable to run parallel requests
LOCALAI_PARALLEL_REQUESTS=true

### Watchdog settings
###
# Enables watchdog to kill backends that are inactive for too much time
LOCALAI_WATCHDOG_IDLE=true
#
# Time in duration format (e.g. 1h30m) after which a backend is considered idle
LOCALAI_WATCHDOG_IDLE_TIMEOUT=45m
#
# Enables watchdog to kill backends that are busy for too much time
LOCALAI_WATCHDOG_BUSY=true
#
# Time in duration format (e.g. 1h30m) after which a backend is considered busy
LOCALAI_WATCHDOG_BUSY_TIMEOUT=15m
